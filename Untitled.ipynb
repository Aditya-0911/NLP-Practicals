{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e37762c-bd9f-44f4-a56e-0d22b84aa371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.13.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.45.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.1.0+cu118)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.5)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.34.2)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.25.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.8)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.13.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install peft scikit-learn transformers pandas datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d1303bc-9dc2-4cbb-b22f-254b9897b7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.metrics import f1_score\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a48ffe24-092b-43e6-b8bd-7075dba0da25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f859fe1-1e5d-414b-b889-e618c7007fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stratify'] = df['file_extension'] + '_' + df['label'].astype(str)\n",
    "train, val = train_test_split(df, test_size=0.075, stratify=df['stratify'], random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "600e36bd-7758-485d-989c-b6e73e79149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[[ 'code','file_extension', 'label']]\n",
    "val = val[['code','file_extension', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a6e561c-9761-4865-92ef-302c2062e564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(161493, 3)\n",
      "(13095, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a635bda5-8962-42ae-bdd7-565172efd8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/graphcodebert-base')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('microsoft/graphcodebert-base', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ca40145-385f-4d61-8661-ac5137e0e5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "roberta\n",
      "roberta.embeddings\n",
      "roberta.embeddings.word_embeddings\n",
      "roberta.embeddings.position_embeddings\n",
      "roberta.embeddings.token_type_embeddings\n",
      "roberta.embeddings.LayerNorm\n",
      "roberta.embeddings.dropout\n",
      "roberta.encoder\n",
      "roberta.encoder.layer\n",
      "roberta.encoder.layer.0\n",
      "roberta.encoder.layer.0.attention\n",
      "roberta.encoder.layer.0.attention.self\n",
      "roberta.encoder.layer.0.attention.self.query\n",
      "roberta.encoder.layer.0.attention.self.key\n",
      "roberta.encoder.layer.0.attention.self.value\n",
      "roberta.encoder.layer.0.attention.self.dropout\n",
      "roberta.encoder.layer.0.attention.output\n",
      "roberta.encoder.layer.0.attention.output.dense\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm\n",
      "roberta.encoder.layer.0.attention.output.dropout\n",
      "roberta.encoder.layer.0.intermediate\n",
      "roberta.encoder.layer.0.intermediate.dense\n",
      "roberta.encoder.layer.0.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.0.output\n",
      "roberta.encoder.layer.0.output.dense\n",
      "roberta.encoder.layer.0.output.LayerNorm\n",
      "roberta.encoder.layer.0.output.dropout\n",
      "roberta.encoder.layer.1\n",
      "roberta.encoder.layer.1.attention\n",
      "roberta.encoder.layer.1.attention.self\n",
      "roberta.encoder.layer.1.attention.self.query\n",
      "roberta.encoder.layer.1.attention.self.key\n",
      "roberta.encoder.layer.1.attention.self.value\n",
      "roberta.encoder.layer.1.attention.self.dropout\n",
      "roberta.encoder.layer.1.attention.output\n",
      "roberta.encoder.layer.1.attention.output.dense\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm\n",
      "roberta.encoder.layer.1.attention.output.dropout\n",
      "roberta.encoder.layer.1.intermediate\n",
      "roberta.encoder.layer.1.intermediate.dense\n",
      "roberta.encoder.layer.1.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.1.output\n",
      "roberta.encoder.layer.1.output.dense\n",
      "roberta.encoder.layer.1.output.LayerNorm\n",
      "roberta.encoder.layer.1.output.dropout\n",
      "roberta.encoder.layer.2\n",
      "roberta.encoder.layer.2.attention\n",
      "roberta.encoder.layer.2.attention.self\n",
      "roberta.encoder.layer.2.attention.self.query\n",
      "roberta.encoder.layer.2.attention.self.key\n",
      "roberta.encoder.layer.2.attention.self.value\n",
      "roberta.encoder.layer.2.attention.self.dropout\n",
      "roberta.encoder.layer.2.attention.output\n",
      "roberta.encoder.layer.2.attention.output.dense\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm\n",
      "roberta.encoder.layer.2.attention.output.dropout\n",
      "roberta.encoder.layer.2.intermediate\n",
      "roberta.encoder.layer.2.intermediate.dense\n",
      "roberta.encoder.layer.2.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.2.output\n",
      "roberta.encoder.layer.2.output.dense\n",
      "roberta.encoder.layer.2.output.LayerNorm\n",
      "roberta.encoder.layer.2.output.dropout\n",
      "roberta.encoder.layer.3\n",
      "roberta.encoder.layer.3.attention\n",
      "roberta.encoder.layer.3.attention.self\n",
      "roberta.encoder.layer.3.attention.self.query\n",
      "roberta.encoder.layer.3.attention.self.key\n",
      "roberta.encoder.layer.3.attention.self.value\n",
      "roberta.encoder.layer.3.attention.self.dropout\n",
      "roberta.encoder.layer.3.attention.output\n",
      "roberta.encoder.layer.3.attention.output.dense\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm\n",
      "roberta.encoder.layer.3.attention.output.dropout\n",
      "roberta.encoder.layer.3.intermediate\n",
      "roberta.encoder.layer.3.intermediate.dense\n",
      "roberta.encoder.layer.3.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.3.output\n",
      "roberta.encoder.layer.3.output.dense\n",
      "roberta.encoder.layer.3.output.LayerNorm\n",
      "roberta.encoder.layer.3.output.dropout\n",
      "roberta.encoder.layer.4\n",
      "roberta.encoder.layer.4.attention\n",
      "roberta.encoder.layer.4.attention.self\n",
      "roberta.encoder.layer.4.attention.self.query\n",
      "roberta.encoder.layer.4.attention.self.key\n",
      "roberta.encoder.layer.4.attention.self.value\n",
      "roberta.encoder.layer.4.attention.self.dropout\n",
      "roberta.encoder.layer.4.attention.output\n",
      "roberta.encoder.layer.4.attention.output.dense\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm\n",
      "roberta.encoder.layer.4.attention.output.dropout\n",
      "roberta.encoder.layer.4.intermediate\n",
      "roberta.encoder.layer.4.intermediate.dense\n",
      "roberta.encoder.layer.4.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.4.output\n",
      "roberta.encoder.layer.4.output.dense\n",
      "roberta.encoder.layer.4.output.LayerNorm\n",
      "roberta.encoder.layer.4.output.dropout\n",
      "roberta.encoder.layer.5\n",
      "roberta.encoder.layer.5.attention\n",
      "roberta.encoder.layer.5.attention.self\n",
      "roberta.encoder.layer.5.attention.self.query\n",
      "roberta.encoder.layer.5.attention.self.key\n",
      "roberta.encoder.layer.5.attention.self.value\n",
      "roberta.encoder.layer.5.attention.self.dropout\n",
      "roberta.encoder.layer.5.attention.output\n",
      "roberta.encoder.layer.5.attention.output.dense\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm\n",
      "roberta.encoder.layer.5.attention.output.dropout\n",
      "roberta.encoder.layer.5.intermediate\n",
      "roberta.encoder.layer.5.intermediate.dense\n",
      "roberta.encoder.layer.5.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.5.output\n",
      "roberta.encoder.layer.5.output.dense\n",
      "roberta.encoder.layer.5.output.LayerNorm\n",
      "roberta.encoder.layer.5.output.dropout\n",
      "roberta.encoder.layer.6\n",
      "roberta.encoder.layer.6.attention\n",
      "roberta.encoder.layer.6.attention.self\n",
      "roberta.encoder.layer.6.attention.self.query\n",
      "roberta.encoder.layer.6.attention.self.key\n",
      "roberta.encoder.layer.6.attention.self.value\n",
      "roberta.encoder.layer.6.attention.self.dropout\n",
      "roberta.encoder.layer.6.attention.output\n",
      "roberta.encoder.layer.6.attention.output.dense\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm\n",
      "roberta.encoder.layer.6.attention.output.dropout\n",
      "roberta.encoder.layer.6.intermediate\n",
      "roberta.encoder.layer.6.intermediate.dense\n",
      "roberta.encoder.layer.6.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.6.output\n",
      "roberta.encoder.layer.6.output.dense\n",
      "roberta.encoder.layer.6.output.LayerNorm\n",
      "roberta.encoder.layer.6.output.dropout\n",
      "roberta.encoder.layer.7\n",
      "roberta.encoder.layer.7.attention\n",
      "roberta.encoder.layer.7.attention.self\n",
      "roberta.encoder.layer.7.attention.self.query\n",
      "roberta.encoder.layer.7.attention.self.key\n",
      "roberta.encoder.layer.7.attention.self.value\n",
      "roberta.encoder.layer.7.attention.self.dropout\n",
      "roberta.encoder.layer.7.attention.output\n",
      "roberta.encoder.layer.7.attention.output.dense\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm\n",
      "roberta.encoder.layer.7.attention.output.dropout\n",
      "roberta.encoder.layer.7.intermediate\n",
      "roberta.encoder.layer.7.intermediate.dense\n",
      "roberta.encoder.layer.7.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.7.output\n",
      "roberta.encoder.layer.7.output.dense\n",
      "roberta.encoder.layer.7.output.LayerNorm\n",
      "roberta.encoder.layer.7.output.dropout\n",
      "roberta.encoder.layer.8\n",
      "roberta.encoder.layer.8.attention\n",
      "roberta.encoder.layer.8.attention.self\n",
      "roberta.encoder.layer.8.attention.self.query\n",
      "roberta.encoder.layer.8.attention.self.key\n",
      "roberta.encoder.layer.8.attention.self.value\n",
      "roberta.encoder.layer.8.attention.self.dropout\n",
      "roberta.encoder.layer.8.attention.output\n",
      "roberta.encoder.layer.8.attention.output.dense\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm\n",
      "roberta.encoder.layer.8.attention.output.dropout\n",
      "roberta.encoder.layer.8.intermediate\n",
      "roberta.encoder.layer.8.intermediate.dense\n",
      "roberta.encoder.layer.8.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.8.output\n",
      "roberta.encoder.layer.8.output.dense\n",
      "roberta.encoder.layer.8.output.LayerNorm\n",
      "roberta.encoder.layer.8.output.dropout\n",
      "roberta.encoder.layer.9\n",
      "roberta.encoder.layer.9.attention\n",
      "roberta.encoder.layer.9.attention.self\n",
      "roberta.encoder.layer.9.attention.self.query\n",
      "roberta.encoder.layer.9.attention.self.key\n",
      "roberta.encoder.layer.9.attention.self.value\n",
      "roberta.encoder.layer.9.attention.self.dropout\n",
      "roberta.encoder.layer.9.attention.output\n",
      "roberta.encoder.layer.9.attention.output.dense\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm\n",
      "roberta.encoder.layer.9.attention.output.dropout\n",
      "roberta.encoder.layer.9.intermediate\n",
      "roberta.encoder.layer.9.intermediate.dense\n",
      "roberta.encoder.layer.9.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.9.output\n",
      "roberta.encoder.layer.9.output.dense\n",
      "roberta.encoder.layer.9.output.LayerNorm\n",
      "roberta.encoder.layer.9.output.dropout\n",
      "roberta.encoder.layer.10\n",
      "roberta.encoder.layer.10.attention\n",
      "roberta.encoder.layer.10.attention.self\n",
      "roberta.encoder.layer.10.attention.self.query\n",
      "roberta.encoder.layer.10.attention.self.key\n",
      "roberta.encoder.layer.10.attention.self.value\n",
      "roberta.encoder.layer.10.attention.self.dropout\n",
      "roberta.encoder.layer.10.attention.output\n",
      "roberta.encoder.layer.10.attention.output.dense\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm\n",
      "roberta.encoder.layer.10.attention.output.dropout\n",
      "roberta.encoder.layer.10.intermediate\n",
      "roberta.encoder.layer.10.intermediate.dense\n",
      "roberta.encoder.layer.10.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.10.output\n",
      "roberta.encoder.layer.10.output.dense\n",
      "roberta.encoder.layer.10.output.LayerNorm\n",
      "roberta.encoder.layer.10.output.dropout\n",
      "roberta.encoder.layer.11\n",
      "roberta.encoder.layer.11.attention\n",
      "roberta.encoder.layer.11.attention.self\n",
      "roberta.encoder.layer.11.attention.self.query\n",
      "roberta.encoder.layer.11.attention.self.key\n",
      "roberta.encoder.layer.11.attention.self.value\n",
      "roberta.encoder.layer.11.attention.self.dropout\n",
      "roberta.encoder.layer.11.attention.output\n",
      "roberta.encoder.layer.11.attention.output.dense\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm\n",
      "roberta.encoder.layer.11.attention.output.dropout\n",
      "roberta.encoder.layer.11.intermediate\n",
      "roberta.encoder.layer.11.intermediate.dense\n",
      "roberta.encoder.layer.11.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.11.output\n",
      "roberta.encoder.layer.11.output.dense\n",
      "roberta.encoder.layer.11.output.LayerNorm\n",
      "roberta.encoder.layer.11.output.dropout\n",
      "classifier\n",
      "classifier.dense\n",
      "classifier.dropout\n",
      "classifier.out_proj\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a49a741c-940f-462e-92fd-1653db03bb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lora_config = LoraConfig(\n",
    "#     r=8,  # LoRA rank\n",
    "#     lora_alpha=16,\n",
    "#     lora_dropout=0.1,\n",
    "#     target_modules=['query', 'key', 'value'],  # Apply LoRA to specific model layers\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"SEQ_CLS\"  # Sequence classification task\n",
    "# )\n",
    "\n",
    "# # Apply LoRA to the base model\n",
    "# model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3eb5453-cc67-4dc7-a3ca-6fbec3b07791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    input_texts = []\n",
    "    \n",
    "    # Process each 'code' and 'file_extension' in the batch\n",
    "    for code, file_extension in zip(examples['code'], examples['file_extension']):\n",
    "        # Split the 'code' into lines\n",
    "        lines = code.split('\\n')\n",
    "        \n",
    "        # Extract the third line (C) if available\n",
    "        third_line = lines[2] if len(lines) > 2 else ''\n",
    "        \n",
    "        # Concatenate relevant parts: full code, third line, and file_extension\n",
    "        input_text = code + \" \" + third_line + \" \" + file_extension\n",
    "        input_texts.append(input_text)\n",
    "    \n",
    "    # Tokenize using DistilBERT's tokenizer for the entire batch\n",
    "    tokenized_inputs = tokenizer(input_texts, padding='max_length', truncation=True, max_length=512)\n",
    "    \n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e846a69a-e42b-4392-aa40-70d7533c879d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cde49c719ca487ea3d2d62bcea17e15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/161493 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de13531eaa884d178c220a71d7bd38e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13095 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = Dataset.from_pandas(train)  # Your pre-split training data\n",
    "val_dataset = Dataset.from_pandas(val)      # Your pre-split validation data\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb2ee99c-c1d7-40fe-8059-a6ad47fa1f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # Convert logits to a PyTorch tensor if they are not already\n",
    "    if isinstance(logits, np.ndarray):\n",
    "        logits = torch.tensor(logits)\n",
    "    \n",
    "    preds = torch.argmax(logits, axis=-1)\n",
    "    f1 = f1_score(labels, preds.numpy(), average='weighted')  # Convert back to numpy for f1_score\n",
    "    return {'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8328ff5-3846-4a14-9f3c-0807cbab962c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15141' max='15141' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15141/15141 51:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.468400</td>\n",
       "      <td>0.473345</td>\n",
       "      <td>0.757358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.406800</td>\n",
       "      <td>0.449443</td>\n",
       "      <td>0.783796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.317300</td>\n",
       "      <td>0.475492</td>\n",
       "      <td>0.790689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15141, training_loss=0.41658092674132924, metrics={'train_runtime': 3099.1781, 'train_samples_per_second': 156.325, 'train_steps_per_second': 4.885, 'total_flos': 1.2747178098975744e+17, 'train_loss': 0.41658092674132924, 'epoch': 3.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    eval_strategy='epoch',\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50, \n",
    "    load_best_model_at_end=True,\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    eval_steps=100,\n",
    "    save_total_limit=2,\n",
    "    fp16 = True\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4db73f3f-756b-4307-bf3d-4735344a468b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./graphcodebert/tokenizer_config.json',\n",
       " './graphcodebert/special_tokens_map.json',\n",
       " './graphcodebert/vocab.json',\n",
       " './graphcodebert/merges.txt',\n",
       " './graphcodebert/added_tokens.json',\n",
       " './graphcodebert/tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_directory = \"./graphcodebert\"\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26272262-bf46-44b5-ac8a-d96e3e846951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('test.csv') \n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('graphcodebert')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('graphcodebert')\n",
    "\n",
    "# Make sure the model is in evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82967ae9-94e2-4d32-a0b6-f260f2d61ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3628/3628 [03:12<00:00, 18.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created: submission.csv\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('graphcodebert')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('graphcodebert')\n",
    "\n",
    "# Make sure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Function to preprocess input examples for the model\n",
    "def preprocess_function(examples):\n",
    "    input_texts = []\n",
    "    \n",
    "    for code, file_extension in zip(examples['code'], examples['file_extension']):\n",
    "        # Split the code into lines\n",
    "        lines = code.split('\\n')\n",
    "        \n",
    "        # Extract the third line (C) if available\n",
    "        third_line = lines[2] if len(lines) > 2 else ''\n",
    "        \n",
    "        # Concatenate relevant parts: full code, third line, and file_extension\n",
    "        input_text = code + \" \" + third_line + \" \" + file_extension\n",
    "        input_texts.append(input_text)\n",
    "    \n",
    "    # Tokenize using DistilBERT's tokenizer\n",
    "    tokenized_inputs = tokenizer(input_texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Move the model to the correct device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Initialize an empty list for the predictions\n",
    "predictions = []\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 16  # You can adjust this based on your GPU memory availability\n",
    "\n",
    "# Loop through the test dataset in batches\n",
    "for i in tqdm(range(0, len(test_df), batch_size)):\n",
    "    batch_df = test_df.iloc[i:i + batch_size]\n",
    "    \n",
    "    # Preprocess the batch\n",
    "    test_inputs = preprocess_function(batch_df)\n",
    "    \n",
    "    # Move tensors to the correct device (CPU or GPU)\n",
    "    test_inputs = {k: v.to(device) for k, v in test_inputs.items()}\n",
    "    \n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**test_inputs)\n",
    "        batch_predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    # Convert predictions to CPU and add to the list\n",
    "    predictions.extend(batch_predictions.cpu().numpy())\n",
    "    \n",
    "    # Clear the cache to avoid memory overflow\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'label': predictions\n",
    "})\n",
    "\n",
    "# Save the submission file to CSV\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Submission file created: submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3607f6af-88a7-4d32-98fd-c377d80f1fed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
